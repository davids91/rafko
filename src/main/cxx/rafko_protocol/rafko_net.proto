/*! This file is part of davids91/Rafko.
 *
 *    Rafko is free software: you can redistribute it and/or modify
 *    it under the terms of the GNU General Public License as published by
 *    the Free Software Foundation, either version 3 of the License, or
 *    (at your option) any later version.
 *
 *    Rafko is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU General Public License for more details.
 *
 *    You should have received a copy of the GNU General Public License
 *    along with Rafko.  If not, see <https://www.gnu.org/licenses/> or
 *    <https://github.com/davids91/rafko/blob/master/LICENSE>
 */

syntax = "proto3";
option cc_enable_arenas = true;
option java_package = "org.rafko.rafko_net";
option java_outer_classname = "RafkoNet";
package rafko_net;

/** @brief      Indexes of the transfer function a neuron is able to use
 *              on the weigthed sum of its operation
 */
enum Transfer_functions{
  transfer_function_unknown = 0;
  transfer_function_identity = 1;
  transfer_function_sigmoid = 2;
  transfer_function_tanh = 3;
  transfer_function_elu = 4;
  transfer_function_selu = 5;
  transfer_function_relu = 6;
  transfer_function_end = 512;
}

/** @brief      Network basically describes the connections a recurrent network has
 *              with the past runs. This enumeration is for a utility which is for
 *              testing purposes only. Recurrences described here affect
 *              only the rpc @build_network in the service @RafkoDeepLearning
 */
enum Network_recurrence{
  network_recurrence_unknown = 0;
  network_recurrence_to_self = 1;
  network_recurrence_to_layer = 2;
}

/** @brief      Different features a group of Neurons(may even the whole network) might have.
 */
enum Neuron_group_features{
  neuron_group_feature_unknown = 0;
  neuron_group_feature_softmax = 1;
  neuron_group_feature_disentanglement = 2;
  neuron_group_feature_dropout_regularization = 3;
}

/**
 * @brief      These classes describes a synapse each. A synapse corresponds with a table of intervals.
 *             The number of @starts and @sizes should always be equal. Each pair of them describes
 *             an index interval where @starts[x] states where the interval x is starting, and @sizes[x]
 *             denotes the number of indices present in taht interval. The intervals are free to overlap
 *             or coincide.
 *             The element @starts can be of negative number. The sign of it denotes a different kind of iteration
 *             on the given interval. In case the sign is negative for that element, the interval it describes grows
 *             in the negative direction. This way two kinds of intervals can be stored in the same synapse.
 *             Positive interval starts at 0, goes on indefinitely: [0,+inf]
 *             -Positive intervals usually denote enclosed relationships (e.g.: internal neurons in a @PartialSolution)
 *             Negative interval starts at -1, goes backwards indefinitely: [-1,-inf]
 *             - Negative intervals usually denote external relationships (e.g.: input data of the Neurons in the @Net
 *               or inputs of an Inner Neuron, taken from the @PartialSolution input synapses )
 *             In the class @SynapseIterator there are methods to map negative intervals into positive array numbers.
 *             The -1 becomes index 0, -2 ==> 1 and so on... This is needed to be able to use negative intervals as
 *             indexes in arrays.
 */
message InputSynapseInterval{
  uint32 reach_past_loops = 1; /* How far in the past runs of the network is taken from. 0 is current run. */
  sint32 starts = 10; /* Starting indexes of intervals */
  uint32 interval_size = 11; /* Sizes of intervals */
}

message IndexSynapseInterval{
  sint32 starts = 10; /* Starting indexes of intervals */
  uint32 interval_size = 11; /* Sizes of intervals */
}

/** @brief      Takes input, assigns weights to them and processes
 *              the weighted sums with a bias, transfer function and a memory filter.
 */
message Neuron{
  /* index of the weight for the memory filter. 1.0: keep its previous value instead of the newly calculated one */
  Transfer_functions transfer_function_idx = 3; /* index of the transfer function this Neuron uses, empty is invalid */

  /**
   * Input weights shall only contain positive intervals,
   * each interval corresponding to an index in the @weight_table in the @RafkoNet
   * Every weight above inputs in @input_indices count as a bias, and shall behave
   * like it has an input of 1.
   */
  repeated IndexSynapseInterval input_weights = 10;

  /**
   * Input indices describes intervals denoting the neurons inputs.
   * Each positive interval corresponds to indices in the internal data of a @Neuron stored in the @SolutionSolver
   * Each negative interval corresponds to indices in the input data given to the @SolutionSolver
   */
  repeated InputSynapseInterval input_indices = 11;
}

/** @brief      A message representing a group of Neurons inside the Neural network
 *              associated with a given feature
 */
message FeatureGroup{
  Neuron_group_features feature = 1;
  repeated IndexSynapseInterval relevant_neurons = 10;
}

/** @brief      A sparse net implementation containing the
 *              Neurons and optimization cache files
 */
message RafkoNet{
  uint32 input_data_size = 10; /* Number of inputs (floating point numbers) accepted by the Neural network */
  uint32 output_neuron_number = 11; /* Number of outputs the Neural network has */
  repeated FeatureGroup neuron_group_features = 12; /* Neurons grouped together for features. The same features may be repeated with different neuron indices */

  repeated Neuron neuron_array = 20; /* Array of Neurons the network has */
  repeated double weight_table = 21; /* Stores individual weights used by the Neurons */
}
