/*! This file is part of davids91/Rafko.
 *
 *    Rafko is free software: you can redistribute it and/or modify
 *    it under the terms of the GNU General Public License as published by
 *    the Free Software Foundation, either version 3 of the License, or
 *    (at your option) any later version.
 *
 *    Rafko is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU General Public License for more details.
 *
 *    You should have received a copy of the GNU General Public License
 *    along with Rafko.  If not, see <https://www.gnu.org/licenses/> or
 *    <https://github.com/davids91/rafko/blob/master/LICENSE>
 */

syntax = "proto3";
option cc_enable_arenas = true;
option java_package = "org.rafko.rafko_net";
option java_outer_classname = "RafkoCommon";
package rafko_net;

/** @brief      Indexes of the transfer function a neuron is able to use
 *              on the weigthed sum of its operation
 */
enum Transfer_functions{
  transfer_function_unknown = 0;
  transfer_function_identity = 1;
  transfer_function_sigmoid = 2;
  transfer_function_tanh = 3;
  transfer_function_elu = 4;
  transfer_function_selu = 5;
  transfer_function_relu = 6;
  transfer_function_end = 512;
}

/** @brief      Indexes of the error functions a Neural Network is able to use
 *              to calculate the gradient of the weights
 */
enum Cost_functions{
  cost_function_unknown = 0;
  cost_function_squared_error = 1; /* ( (expected-calculated)^2 ) */
  cost_function_mse = 2; /* ( 0.5*(expected-calculated)^2 )/dataset_size */
}

/** @brief      Describes the available weight updaters implemented in the framework
 */
enum Weight_updaters{
  weight_updater_unknown = 0;
  weight_updater_default = 1;
  weight_updater_momentum = 2;
  weight_updater_nesterovs = 3;
  weight_updater_adam = 4;
  weight_updater_amsgrad = 5;
  weight_updater_end = 512;
}

/**
 * @brief      These classes describes a synapse each. A synapse corresponds with a table of intervals.
 *             The number of @starts and @sizes should always be equal. Each pair of them describes
 *             an index interval where @starts[x] states where the interval x is starting, and @sizes[x]
 *             denotes the number of indices present in taht interval. The intervals are free to overlap
 *             or coincide.
 *             The element @starts can be of negative number. The sign of it denotes a different kind of iteration
 *             on the given interval. In case the sign is negative for that element, the interval it describes grows
 *             in the negative direction. This way two kinds of intervals can be stored in the same synapse.
 *             Positive interval starts at 0, goes on indefinitely: [0,+inf]
 *             -Positive intervals usually denote enclosed relationships (e.g.: internal neurons in a @PartialSolution)
 *             Negative interval starts at -1, goes backwards indefinitely: [-1,-inf]
 *             - Negative intervals usually denote external relationships (e.g.: input data of the Neurons in the @Net
 *               or inputs of an Inner Neuron, taken from the @PartialSolution input synapses )
 *             In the class @SynapseIterator there are methods to map negative intervals into positive array numbers.
 *             The -1 becomes index 0, -2 ==> 1 and so on... This is needed to be able to use negative intervals as
 *             indexes in arrays.
 */
message InputSynapseInterval{
  uint32 reach_past_loops = 1; /* How far in the past runs of the network is taken from. 0 is current run. */
  sint32 starts = 10; /* Starting indexes of intervals */
  uint32 interval_size = 11; /* Sizes of intervals */
}

message IndexSynapseInterval{
  sint32 starts = 10; /* Starting indexes of intervals */
  uint32 interval_size = 11; /* Sizes of intervals */
}
